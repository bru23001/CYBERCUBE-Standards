# CYBERCUBE Service Level Policy (v1)

**Standard ID:** STD-SLP-001  
**Status:** Active  
**Effective:** 2026-01-17  
**Version:** 1.0  
**Classification:** INTERNAL (SLOs), PUBLIC (SLAs)  
**Owner:** VP Engineering  
**Approver:** CTO  
**Applies to:** All CYBERCUBE production services

---

## Table of Contents

0. [Purpose & Design Principles](#0-purpose--design-principles)
1. [Service Level Indicators (SLIs)](#1-service-level-indicators-slis)
2. [Service Level Objectives (SLOs)](#2-service-level-objectives-slos)
3. [Error Budgets](#3-error-budgets)
4. [Service Level Agreements (SLAs)](#4-service-level-agreements-slas)
5. [Service Credits](#5-service-credits)
6. [Breach Handling](#6-breach-handling)
7. [Monitoring & Reporting](#7-monitoring--reporting)
8. [SLO Lifecycle](#8-slo-lifecycle)
- [Quick Reference Card](#quick-reference-card)
- [Appendix A: Glossary](#appendix-a-glossary)

---

## 0. Purpose & Design Principles

This policy establishes the framework for defining, measuring, and managing
service levels at CYBERCUBE. It governs both internal Service Level Objectives
(SLOs) used for engineering decisions and external Service Level Agreements
(SLAs) committed to customers.

**Industry Alignment:**
- Google SRE Workbook
- ITIL 4 Service Level Management
- ISO/IEC 20000-1
- SOC 2 Type II
- Enterprise SaaS best practices

**Design Principles:**

1. **Measurable** â€” All service levels based on quantifiable metrics
2. **Customer-Centric** â€” SLIs reflect customer experience
3. **Realistic** â€” Targets are achievable and meaningful
4. **Actionable** â€” Breach triggers defined responses
5. **Transparent** â€” Status visible to stakeholders
6. **Balanced** â€” Reliability vs. velocity trade-offs explicit

**Relationship Between SLI, SLO, and SLA:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        SERVICE LEVEL HIERARCHY                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  SLI (Service Level Indicator)                                      â”‚   â”‚
â”‚  â”‚  What we measure                                                    â”‚   â”‚
â”‚  â”‚  Example: "Percentage of requests completing in < 200ms"           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â”‚                                              â”‚
â”‚                              â–¼                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  SLO (Service Level Objective)                                      â”‚   â”‚
â”‚  â”‚  Internal target (stricter)                                         â”‚   â”‚
â”‚  â”‚  Example: "99.95% of requests complete in < 200ms"                 â”‚   â”‚
â”‚  â”‚  Buffer for engineering to catch issues before customer impact     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â”‚                                              â”‚
â”‚                              â–¼                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  SLA (Service Level Agreement)                                      â”‚   â”‚
â”‚  â”‚  External commitment (published)                                    â”‚   â”‚
â”‚  â”‚  Example: "99.9% of requests complete in < 200ms"                  â”‚   â”‚
â”‚  â”‚  Breach â†’ Service credits                                          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚  Key: SLO target > SLA commitment (safety buffer)                         â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1. Service Level Indicators (SLIs)

### 1.1 Core SLI Categories

| Category | Description | Measurement |
|----------|-------------|-------------|
| **Availability** | Service is up and responding | Successful requests / Total requests |
| **Latency** | Response time for requests | Percentile distribution (p50, p95, p99) |
| **Error Rate** | Requests resulting in errors | Failed requests / Total requests |
| **Throughput** | Capacity to handle load | Requests per second |
| **Durability** | Data not lost | Data objects retained / Data objects stored |
| **Freshness** | Data recency | Time since last update |

### 1.2 SLI Specifications

#### 1.2.1 Availability SLI

**Definition:** The proportion of valid requests that are served successfully.

```
Availability = (Total Requests - Failed Requests) / Total Requests Ã— 100%
```

**What counts as "available":**
- HTTP 2xx, 3xx responses
- HTTP 4xx (client errors) â€” NOT counted as unavailable
- Successful API operations

**What counts as "unavailable":**
- HTTP 5xx responses
- Timeouts (> configured threshold)
- Connection failures
- Service unreachable

**Measurement points:**
- Load balancer logs (primary)
- Application metrics (secondary)
- Synthetic monitoring (tertiary)

#### 1.2.2 Latency SLI

**Definition:** The time taken to serve a request, measured at specific percentiles.

**Percentile definitions:**

| Percentile | Meaning | Use Case |
|------------|---------|----------|
| **p50** (median) | 50% of requests faster | Typical user experience |
| **p90** | 90% of requests faster | Most users' experience |
| **p95** | 95% of requests faster | Customer commitment |
| **p99** | 99% of requests faster | Worst-case monitoring |

**Measurement:**
- From request received to response sent
- Excludes network latency to/from client
- Per endpoint or aggregated

#### 1.2.3 Error Rate SLI

**Definition:** The proportion of requests resulting in errors.

```
Error Rate = Failed Requests / Total Requests Ã— 100%
```

**Error classification:**

| Category | Examples | Counts as Error? |
|----------|----------|------------------|
| Server errors | 5xx, timeouts, crashes | Yes |
| Client errors | 4xx (bad request, auth) | No |
| Partial success | Degraded response | Configurable |
| Rate limiting | 429 responses | No (expected behavior) |

#### 1.2.4 Durability SLI

**Definition:** The probability that stored data will not be lost.

```
Durability = (Objects Stored - Objects Lost) / Objects Stored Ã— 100%
```

**Applies to:** Databases, file storage, backups

### 1.3 SLI Implementation

```typescript
// Example SLI calculation
interface SLIConfig {
  name: string;
  type: 'availability' | 'latency' | 'error_rate' | 'throughput';
  window: '5m' | '1h' | '1d' | '30d';
  good_events_query: string;
  total_events_query: string;
  threshold?: number;  // For latency
  percentile?: number; // For latency
}

const availabilitySLI: SLIConfig = {
  name: 'api_availability',
  type: 'availability',
  window: '30d',
  good_events_query: 'sum(http_requests_total{status!~"5.."})',
  total_events_query: 'sum(http_requests_total)',
};

const latencySLI: SLIConfig = {
  name: 'api_latency_p95',
  type: 'latency',
  window: '30d',
  good_events_query: 'sum(http_request_duration_seconds_bucket{le="0.2"})',
  total_events_query: 'sum(http_request_duration_seconds_count)',
  threshold: 0.2,  // 200ms
  percentile: 95,
};
```

---

## 2. Service Level Objectives (SLOs)

### 2.1 Internal SLO Definitions

SLOs are internal targets that engineering teams use to guide reliability
investments. They should be stricter than external SLAs.

#### 2.1.1 Tiered Service Classification

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        SERVICE TIER CLASSIFICATION                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  TIER 1 â€” CRITICAL                                                         â”‚
â”‚  â”œâ”€â”€ Customer-facing core services                                         â”‚
â”‚  â”œâ”€â”€ Authentication / Authorization                                        â”‚
â”‚  â”œâ”€â”€ Payment processing                                                    â”‚
â”‚  â””â”€â”€ Data storage (primary)                                                â”‚
â”‚                                                                             â”‚
â”‚  TIER 2 â€” HIGH                                                              â”‚
â”‚  â”œâ”€â”€ Customer-facing secondary services                                    â”‚
â”‚  â”œâ”€â”€ API gateway                                                           â”‚
â”‚  â”œâ”€â”€ Notifications                                                         â”‚
â”‚  â””â”€â”€ Reporting / Analytics                                                 â”‚
â”‚                                                                             â”‚
â”‚  TIER 3 â€” STANDARD                                                          â”‚
â”‚  â”œâ”€â”€ Internal tools                                                        â”‚
â”‚  â”œâ”€â”€ Admin portals                                                         â”‚
â”‚  â”œâ”€â”€ Background jobs                                                       â”‚
â”‚  â””â”€â”€ Development infrastructure                                            â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 2.1.2 SLO Targets by Tier

| Metric | Tier 1 (Critical) | Tier 2 (High) | Tier 3 (Standard) |
|--------|-------------------|---------------|-------------------|
| **Availability** | 99.95% | 99.9% | 99.5% |
| **Latency (p95)** | < 200ms | < 500ms | < 2000ms |
| **Latency (p99)** | < 500ms | < 1000ms | < 5000ms |
| **Error Rate** | < 0.05% | < 0.1% | < 0.5% |

**Availability in context:**

| Target | Monthly Downtime | Annual Downtime |
|--------|------------------|-----------------|
| 99.99% | 4.3 minutes | 52.6 minutes |
| 99.95% | 21.9 minutes | 4.4 hours |
| 99.9% | 43.8 minutes | 8.8 hours |
| 99.5% | 3.6 hours | 1.8 days |
| 99% | 7.3 hours | 3.7 days |

#### 2.1.3 Service-Specific SLOs

| Service | Availability | Latency (p95) | Error Rate |
|---------|--------------|---------------|------------|
| **Web Application** | 99.95% | 200ms | 0.05% |
| **API Platform** | 99.95% | 150ms | 0.05% |
| **Authentication** | 99.99% | 100ms | 0.01% |
| **Database (Primary)** | 99.99% | 50ms | 0.01% |
| **Database (Replicas)** | 99.9% | 100ms | 0.1% |
| **File Storage** | 99.95% | 500ms | 0.05% |
| **Background Jobs** | 99.5% | N/A | 0.5% |
| **Email Delivery** | 99.9% | N/A | 0.1% |
| **Webhook Delivery** | 99.9% | N/A | 0.1% |
| **Admin Portal** | 99.5% | 500ms | 0.5% |

#### 2.1.4 Data Durability SLOs

| Data Type | Durability Target | RPO | RTO |
|-----------|-------------------|-----|-----|
| **Customer data** | 99.999999999% (11 nines) | 1 hour | 4 hours |
| **Transaction records** | 99.999999999% | 0 (synchronous) | 1 hour |
| **Audit logs** | 99.99999% (7 nines) | 1 hour | 24 hours |
| **User uploads** | 99.99999% | 1 hour | 4 hours |
| **Analytics data** | 99.999% (5 nines) | 24 hours | 48 hours |
| **Temporary data** | 99.9% | N/A | N/A |

### 2.2 SLO Measurement Window

| Window Type | Duration | Use Case |
|-------------|----------|----------|
| **Rolling** | 30 days | Primary measurement |
| **Calendar Month** | 1 month | Billing alignment |
| **Quarterly** | 3 months | Trend analysis |

**Primary measurement:** Rolling 30-day window, calculated continuously.

### 2.3 SLO Documentation Template

```markdown
## SLO Definition: [Service Name]

### Service Information
- **Service:** [Name]
- **Tier:** [1/2/3]
- **Owner:** [Team]
- **Last Review:** [Date]

### SLIs & Targets

| SLI | Target | Measurement |
|-----|--------|-------------|
| Availability | [X]% | [Query/Method] |
| Latency (p95) | [X]ms | [Query/Method] |
| Error Rate | [X]% | [Query/Method] |

### Error Budget

- **Budget (30d):** [X] minutes / [X] requests
- **Current Burn Rate:** [X]x normal
- **Budget Remaining:** [X]%

### Alerting

| Condition | Severity | Response |
|-----------|----------|----------|
| [Threshold] | [P1-P4] | [Action] |

### Dependencies

| Service | Impact if Degraded |
|---------|-------------------|
| [Service] | [Impact] |

### Review History

| Date | Change | Rationale |
|------|--------|-----------|
| [Date] | [Change] | [Why] |
```

---

## 3. Error Budgets

### 3.1 Error Budget Concept

Error budget is the acceptable amount of unreliability within a measurement
period. It quantifies how much failure is tolerable before breaching the SLO.

```
Error Budget = (1 - SLO) Ã— Measurement Window
```

**Examples (30-day window):**

| SLO | Error Budget (Time) | Error Budget (%) |
|-----|---------------------|------------------|
| 99.99% | 4.3 minutes | 0.01% |
| 99.95% | 21.9 minutes | 0.05% |
| 99.9% | 43.8 minutes | 0.1% |
| 99.5% | 3.6 hours | 0.5% |

### 3.2 Error Budget Calculation

```typescript
interface ErrorBudget {
  service: string;
  slo_target: number;           // e.g., 0.9995 for 99.95%
  window_days: number;          // e.g., 30
  total_requests: number;       // In window
  failed_requests: number;      // In window
}

function calculateErrorBudget(budget: ErrorBudget) {
  // Total budget
  const totalBudget = (1 - budget.slo_target) * budget.total_requests;
  
  // Consumed budget
  const consumedBudget = budget.failed_requests;
  
  // Remaining
  const remainingBudget = totalBudget - consumedBudget;
  const remainingPercent = (remainingBudget / totalBudget) * 100;
  
  // Burn rate (1.0 = normal consumption)
  const expectedConsumption = consumedBudget / budget.window_days;
  const normalRate = totalBudget / budget.window_days;
  const burnRate = expectedConsumption / normalRate;
  
  return {
    total: totalBudget,
    consumed: consumedBudget,
    remaining: remainingBudget,
    remainingPercent,
    burnRate,
    daysUntilExhausted: burnRate > 0 ? remainingBudget / (consumedBudget / budget.window_days) : Infinity,
  };
}
```

### 3.3 Burn Rate Alerting

Burn rate indicates how fast error budget is being consumed relative to normal.

| Burn Rate | Meaning | Time to Exhaustion (30d budget) |
|-----------|---------|--------------------------------|
| 1x | Normal | 30 days |
| 2x | Double normal | 15 days |
| 6x | Significant issue | 5 days |
| 14.4x | Critical | 2 days |
| 36x | Severe | ~20 hours |

**Alert thresholds:**

| Alert | Burn Rate | Window | Page? |
|-------|-----------|--------|-------|
| Ticket | 2x | 72h | No |
| Warning | 6x | 6h | No |
| Critical | 14.4x | 1h | Yes |
| Emergency | 36x | 5m | Yes |

### 3.4 Error Budget Policy

#### 3.4.1 Budget Status Levels

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       ERROR BUDGET STATUS LEVELS                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  ğŸŸ¢ GREEN â€” Budget > 50% remaining                                  â”‚   â”‚
â”‚  â”‚  â€¢ Normal feature development                                       â”‚   â”‚
â”‚  â”‚  â€¢ Standard change process                                          â”‚   â”‚
â”‚  â”‚  â€¢ Regular deployment cadence                                       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  ğŸŸ¡ YELLOW â€” Budget 25-50% remaining                                â”‚   â”‚
â”‚  â”‚  â€¢ Increased monitoring attention                                   â”‚   â”‚
â”‚  â”‚  â€¢ Review recent incidents                                          â”‚   â”‚
â”‚  â”‚  â€¢ Prioritize reliability improvements                              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  ğŸŸ  ORANGE â€” Budget 10-25% remaining                                â”‚   â”‚
â”‚  â”‚  â€¢ Feature freeze for this service                                  â”‚   â”‚
â”‚  â”‚  â€¢ Only reliability work                                            â”‚   â”‚
â”‚  â”‚  â€¢ Reduced deployment frequency                                     â”‚   â”‚
â”‚  â”‚  â€¢ Root cause analysis required                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  ğŸ”´ RED â€” Budget < 10% remaining or exhausted                       â”‚   â”‚
â”‚  â”‚  â€¢ Complete feature freeze                                          â”‚   â”‚
â”‚  â”‚  â€¢ Emergency reliability focus                                      â”‚   â”‚
â”‚  â”‚  â€¢ No deployments except fixes                                      â”‚   â”‚
â”‚  â”‚  â€¢ Executive escalation                                             â”‚   â”‚
â”‚  â”‚  â€¢ SLA breach risk assessment                                       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 3.4.2 Actions by Budget Status

| Status | Development | Deployments | Meetings | Escalation |
|--------|-------------|-------------|----------|------------|
| ğŸŸ¢ Green | Normal | Normal | Weekly SLO review | None |
| ğŸŸ¡ Yellow | Continue with caution | Extra validation | Daily standup | Engineering Lead |
| ğŸŸ  Orange | Reliability only | Reliability fixes only | Daily review | Director |
| ğŸ”´ Red | Freeze | Emergency only | Incident mode | Executive |

#### 3.4.3 Budget Reset

- Error budget resets at the start of each measurement window
- No "banking" of unused budget
- Historical budget performance tracked for trends

---

## 4. Service Level Agreements (SLAs)

### 4.1 External SLA Commitments

SLAs are contractual commitments to customers. They must be:
- **Achievable:** Based on proven SLO performance
- **Measurable:** Using defined SLIs
- **Clear:** Unambiguous terms
- **Enforceable:** With defined consequences

**SLA Buffer:** SLA target should be at least one "nine" less than SLO.

```
Example:
  SLO: 99.95% availability
  SLA: 99.9% availability (safer commitment)
```

### 4.2 Published SLA Tiers

#### 4.2.1 Standard SLA (All Plans)

| Metric | Commitment |
|--------|------------|
| **Monthly Uptime** | 99.9% |
| **Latency (p95)** | < 500ms |
| **Data Durability** | 99.99999% |
| **Planned Maintenance** | 8 hours notice, max 4 hours/month |
| **Support Response** | Business hours |

**Monthly Uptime Calculation:**

```
Monthly Uptime % = (Total Minutes - Downtime Minutes) / Total Minutes Ã— 100
```

#### 4.2.2 Professional SLA

| Metric | Commitment |
|--------|------------|
| **Monthly Uptime** | 99.9% |
| **Latency (p95)** | < 500ms |
| **Data Durability** | 99.99999% |
| **Planned Maintenance** | 24 hours notice, max 2 hours/month |
| **Support Response** | < 4 hours (business hours) |

#### 4.2.3 Enterprise SLA

| Metric | Commitment |
|--------|------------|
| **Monthly Uptime** | 99.9% |
| **Latency (p95)** | < 300ms |
| **Data Durability** | 99.999999999% |
| **Planned Maintenance** | 72 hours notice, max 1 hour/month |
| **Support Response** | < 1 hour (24/7) |
| **Dedicated Support** | Named contacts |

### 4.3 SLA Exclusions

The following events do NOT count against SLA calculations:

| Exclusion | Description |
|-----------|-------------|
| **Planned Maintenance** | Scheduled downtime with proper notice |
| **Customer-Caused** | Issues from customer code, configuration, or usage |
| **Force Majeure** | Natural disasters, war, government actions |
| **Third-Party** | Failures of customer's ISP, DNS, etc. |
| **Beta/Preview** | Features marked as beta or preview |
| **API Abuse** | Rate limiting due to excessive requests |
| **Security Response** | Downtime to address security threats |

### 4.4 SLA Measurement

**Measurement period:** Calendar month (UTC)

**Data source:** CYBERCUBE monitoring systems (authoritative)

**Reporting:**
- Monthly SLA report available in customer portal
- Real-time status at status.cybercube.io
- Incident reports for any service disruption

---

## 5. Service Credits

### 5.1 Credit Calculation

When SLA is breached, eligible customers receive service credits.

#### 5.1.1 Availability Credits

| Monthly Uptime | Credit (% of Monthly Fee) |
|----------------|---------------------------|
| â‰¥ 99.9% | 0% (within SLA) |
| 99.0% - 99.9% | 10% |
| 95.0% - 99.0% | 25% |
| 90.0% - 95.0% | 50% |
| < 90.0% | 100% |

*All plans share the same availability SLA (99.9%). Credit percentages apply
equally; absolute credit amounts scale with plan pricing.*

#### 5.1.2 Credit Calculation Example

```
Scenario:
- Plan: Enterprise (99.9% SLA)
- Monthly fee: $10,000
- Month: 30 days = 43,200 minutes
- Downtime: 60 minutes
- Uptime: (43,200 - 60) / 43,200 = 99.86%

Credit Tier: 99.0% - 99.9% â†’ 10%
Credit Amount: $10,000 Ã— 10% = $1,000
```

### 5.2 Credit Request Process

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       SERVICE CREDIT PROCESS                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  1. INCIDENT         2. REPORT           3. VALIDATE                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚ SLA breach   â”‚â”€â”€â–¶â”‚ Customer     â”‚â”€â”€â–¶â”‚ CYBERCUBE    â”‚                   â”‚
â”‚  â”‚ occurs       â”‚   â”‚ submits      â”‚   â”‚ validates    â”‚                   â”‚
â”‚  â”‚              â”‚   â”‚ credit       â”‚   â”‚ claim        â”‚                   â”‚
â”‚  â”‚              â”‚   â”‚ request      â”‚   â”‚              â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                               â”‚                            â”‚
â”‚                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚                      â”‚                        â”‚                 â”‚          â”‚
â”‚                      â–¼                        â–¼                 â–¼          â”‚
â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚               â”‚   APPROVED   â”‚        â”‚   DENIED     â”‚  â”‚   PARTIAL    â”‚  â”‚
â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                      â”‚                        â”‚                 â”‚          â”‚
â”‚                      â–¼                        â–¼                 â–¼          â”‚
â”‚  4. APPLY        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   Notify customer    Notify +          â”‚
â”‚                  â”‚ Credit       â”‚   with reason        apply partial     â”‚
â”‚                  â”‚ applied to   â”‚                                        â”‚
â”‚                  â”‚ next invoice â”‚                                        â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.3 Credit Terms

| Term | Requirement |
|------|-------------|
| **Request Window** | Within 30 days of incident |
| **Request Method** | Support ticket or portal |
| **Maximum Credit** | 100% of monthly fee (per month) |
| **Credit Form** | Applied to future invoice (no cash refunds) |
| **Exclusions** | Do not apply to excluded events |
| **Documentation** | Customer must describe impact |

### 5.4 Credit Request Template

```markdown
## Service Credit Request

**Customer:** [Company Name]
**Account ID:** [CC-CID]
**Contact:** [Name, Email]
**Date:** [Date]

### Incident Details

**Incident Date(s):** [Date/Time range]
**Service(s) Affected:** [Services]
**Duration:** [Total downtime]

**Description of Impact:**
[How the outage affected your business]

**Reference:**
- CYBERCUBE Incident ID: [If known]
- Status page link: [URL]
- Support ticket: [If applicable]

### Requested Credit

**SLA Tier:** [Standard/Professional/Enterprise]
**Monthly Fee:** [$X]
**Calculated Uptime:** [X%]
**Requested Credit:** [$X or X%]

### Supporting Information

[Any additional details, logs, or evidence]
```

---

## 6. Breach Handling

### 6.1 SLO Breach (Internal)

SLO breach = Internal target missed, SLA may still be met.

**Response Process:**

| Step | Action | Owner | Timing |
|------|--------|-------|--------|
| 1 | Detect breach | Monitoring | Automatic |
| 2 | Alert team | On-call | Immediate |
| 3 | Assess impact | Engineering | < 1 hour |
| 4 | Initiate error budget policy | Engineering Lead | < 4 hours |
| 5 | Root cause analysis | Service Owner | < 24 hours |
| 6 | Remediation plan | Service Owner | < 48 hours |
| 7 | Review at SLO meeting | Team | Weekly |

### 6.2 SLA Breach (External)

SLA breach = Contractual commitment missed, credits may apply.

**Response Process:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        SLA BREACH RESPONSE                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  IMMEDIATE (0-1 hour)                                                      â”‚
â”‚  â”œâ”€â”€ Incident response activated                                           â”‚
â”‚  â”œâ”€â”€ Customer communication initiated                                      â”‚
â”‚  â”œâ”€â”€ Status page updated                                                   â”‚
â”‚  â””â”€â”€ Executive notification                                                â”‚
â”‚                                                                             â”‚
â”‚  SHORT-TERM (1-24 hours)                                                   â”‚
â”‚  â”œâ”€â”€ Service restored                                                      â”‚
â”‚  â”œâ”€â”€ Impact assessment completed                                           â”‚
â”‚  â”œâ”€â”€ Affected customers identified                                         â”‚
â”‚  â””â”€â”€ Initial RCA started                                                   â”‚
â”‚                                                                             â”‚
â”‚  FOLLOW-UP (1-7 days)                                                      â”‚
â”‚  â”œâ”€â”€ Full RCA completed                                                    â”‚
â”‚  â”œâ”€â”€ Customer incident report published                                    â”‚
â”‚  â”œâ”€â”€ Credit calculations performed                                         â”‚
â”‚  â”œâ”€â”€ Remediation actions identified                                        â”‚
â”‚  â””â”€â”€ Process improvements documented                                       â”‚
â”‚                                                                             â”‚
â”‚  LONG-TERM (1-4 weeks)                                                     â”‚
â”‚  â”œâ”€â”€ Remediation implemented                                               â”‚
â”‚  â”œâ”€â”€ Credits applied                                                       â”‚
â”‚  â”œâ”€â”€ SLO/SLA review                                                        â”‚
â”‚  â””â”€â”€ Preventive measures deployed                                          â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.3 Customer Communication

#### 6.3.1 During Incident

| Stage | Communication | Channel |
|-------|---------------|---------|
| Detection | Status page update | Public |
| Ongoing | Updates every 30 min | Status page |
| Resolution | All-clear notice | Status page, email |

#### 6.3.2 Post-Incident

| Timing | Communication | Audience |
|--------|---------------|----------|
| < 24 hours | Initial incident summary | Affected customers |
| < 72 hours | Full incident report | Affected customers |
| < 7 days | Credit notification (if applicable) | Eligible customers |

#### 6.3.3 Incident Report Template

```markdown
## Incident Report: [Title]

**Incident ID:** INC-2026-00001
**Date:** [Date]
**Duration:** [X hours Y minutes]
**Severity:** [SEV-1/2/3/4]
**Services Affected:** [List]

### Summary

[Brief description of what happened and impact]

### Timeline (UTC)

| Time | Event |
|------|-------|
| HH:MM | [Event] |
| HH:MM | [Event] |

### Impact

- **Customers Affected:** [Number/Percentage]
- **Requests Failed:** [Number/Percentage]
- **Data Impact:** [None/Description]

### Root Cause

[Explanation of why the incident occurred]

### Resolution

[How the incident was resolved]

### Preventive Measures

| Action | Status | ETA |
|--------|--------|-----|
| [Action] | [In Progress/Complete] | [Date] |

### SLA Impact

- **SLA Commitment:** [X%]
- **Actual Performance:** [X%]
- **Credit Applicable:** [Yes/No]
```

### 6.4 Escalation Matrix

| Condition | Escalation | Contact |
|-----------|------------|---------|
| SLO breach (single service) | Engineering Lead | [Contact] |
| SLO breach (multiple services) | Director of Engineering | [Contact] |
| SLA at risk (< 10% buffer) | VP Engineering | [Contact] |
| SLA breach confirmed | CTO + CEO | [Contact] |
| Major SLA breach (> 1 hour) | Executive team | [Contact] |

---

## 7. Monitoring & Reporting

### 7.1 SLO Dashboard Requirements

Every service must have a dashboard showing:

| Metric | Visualization | Alert Threshold |
|--------|---------------|-----------------|
| Current SLI values | Real-time gauge | Per SLO |
| SLO compliance (30d) | Time series | < 100% |
| Error budget remaining | Gauge + trend | < 50%, 25%, 10% |
| Burn rate | Time series | 2x, 6x, 14.4x |
| Incident correlation | Timeline | Any |

### 7.2 Alerting Configuration

```yaml
# Example SLO alerting rules
groups:
  - name: slo_alerts
    rules:
      # Error budget warning
      - alert: ErrorBudgetWarning
        expr: |
          1 - (
            sum(rate(http_requests_total{status!~"5.."}[30d]))
            /
            sum(rate(http_requests_total[30d]))
          ) > 0.0005 * 0.5  # 50% of 99.95% budget consumed
        labels:
          severity: warning
        annotations:
          summary: "Error budget 50% consumed for {{ $labels.service }}"
          
      # High burn rate (will exhaust in 2 days)
      - alert: HighBurnRate
        expr: |
          (
            sum(rate(http_requests_total{status=~"5.."}[1h]))
            /
            sum(rate(http_requests_total[1h]))
          ) > (1 - 0.9995) * 14.4
        labels:
          severity: critical
        annotations:
          summary: "High error burn rate for {{ $labels.service }}"
```

### 7.3 Regular Reviews

| Review | Frequency | Participants | Outputs |
|--------|-----------|--------------|---------|
| SLO Standup | Weekly | Service owners | Budget status, action items |
| SLA Review | Monthly | Engineering, Product | Monthly report, trends |
| Quarterly Review | Quarterly | Leadership | Strategic adjustments |
| Annual Review | Annually | Executive | SLO/SLA target revisions |

### 7.4 Reporting

#### 7.4.1 Internal Reports

| Report | Frequency | Audience | Content |
|--------|-----------|----------|---------|
| SLO Status | Weekly | Engineering | All SLOs, budgets, incidents |
| Error Budget | Weekly | Product | Budget consumption, feature impact |
| Reliability | Monthly | Leadership | Trends, investments, risks |

#### 7.4.2 External Reports

| Report | Frequency | Audience | Content |
|--------|-----------|----------|---------|
| Status Page | Real-time | Public | Current status, incidents |
| SLA Report | Monthly | Customers | Uptime, latency, incidents |
| Incident Reports | Per incident | Affected customers | RCA, remediation |

---

## 8. SLO Lifecycle

### 8.1 Creating New SLOs

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        NEW SLO CREATION PROCESS                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  1. PROPOSE           2. MEASURE          3. BASELINE                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚ Identify     â”‚â”€â”€â–¶â”‚ Implement    â”‚â”€â”€â–¶â”‚ Collect 30   â”‚                   â”‚
â”‚  â”‚ SLIs that    â”‚   â”‚ measurement  â”‚   â”‚ days of      â”‚                   â”‚
â”‚  â”‚ matter       â”‚   â”‚              â”‚   â”‚ baseline     â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                               â”‚                            â”‚
â”‚                                               â–¼                            â”‚
â”‚  4. SET TARGET       5. IMPLEMENT         6. ITERATE                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚ Define SLO   â”‚â”€â”€â–¶â”‚ Dashboard,   â”‚â”€â”€â–¶â”‚ Review and   â”‚                   â”‚
â”‚  â”‚ based on     â”‚   â”‚ alerting,    â”‚   â”‚ adjust       â”‚                   â”‚
â”‚  â”‚ baseline +   â”‚   â”‚ error        â”‚   â”‚ quarterly    â”‚                   â”‚
â”‚  â”‚ aspirations  â”‚   â”‚ budget       â”‚   â”‚              â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 8.2 Modifying SLOs

| Reason | Process | Approval |
|--------|---------|----------|
| Tightening (more strict) | Proposal + 30-day baseline | Engineering Lead |
| Loosening (less strict) | Justification + impact analysis | Director + Product |
| New metric | Full creation process | Engineering Lead |
| Retiring | Deprecation notice, 30 days | Service Owner |

### 8.3 SLO-to-SLA Promotion

When an internal SLO should become an external SLA commitment:

1. **Demonstrate stability** â€” SLO met for 6+ consecutive months
2. **Buffer analysis** â€” Ensure SLO > SLA by appropriate margin
3. **Legal review** â€” Terms and credit structure
4. **Customer communication** â€” Announce new SLA
5. **Monitoring update** â€” Add SLA breach alerting

---

## Quick Reference Card

Print it. Keep it handy.

### SLI â†’ SLO â†’ SLA

```
SLI: What we measure
SLO: Internal target (stricter)
SLA: Customer commitment (with buffer)
```

### Availability Targets

| Tier | SLO | SLA | Monthly Budget |
|------|-----|-----|----------------|
| Critical | 99.95% | 99.9% | 22 min / 44 min |
| High | 99.9% | 99.5% | 44 min / 3.6 hr |
| Standard | 99.5% | 99% | 3.6 hr / 7.3 hr |

### Error Budget Status

| Status | Budget | Action |
|--------|--------|--------|
| ğŸŸ¢ Green | > 50% | Normal |
| ğŸŸ¡ Yellow | 25-50% | Caution |
| ğŸŸ  Orange | 10-25% | Feature freeze |
| ğŸ”´ Red | < 10% | Emergency |

### Burn Rate Alerts

| Rate | Exhaustion | Response |
|------|------------|----------|
| 2x | 15 days | Ticket |
| 6x | 5 days | Warning |
| 14.4x | 2 days | Page |
| 36x | 20 hours | Emergency |

### Service Credits

| Uptime | Credit |
|--------|--------|
| < 99.9% | 10% |
| < 99.0% | 25% |
| < 95.0% | 50% |
| < 90.0% | 100% |

### Key Contacts

```
SLO Owner: [Service team]
SLA Questions: [Support]
Status Page: status.cybercube.io
```

---

## Implementation Status

**Last Updated:** 2026-01-17  
**Policy Version:** v1

### Core Implementation

| Component | Status | Notes |
|-----------|--------|-------|
| SLI definitions | COMPLETE | This policy |
| SLO targets | COMPLETE | Per service |
| SLA terms | PARTIAL | Finalize legal review |
| Error budget tracking | PENDING | Implement dashboards |
| Burn rate alerting | PENDING | Configure alerts |
| Service credits process | PENDING | Billing integration |
| Status page | PARTIAL | Enhance automation |
| Customer reporting | PENDING | Portal integration |
| Review cadence | PENDING | Schedule meetings |

---

## Document Approval

| Role | Name | Signature | Date |
|------|------|-----------|------|
| Author | Engineering Operations | â€” | 2026-01-17 |
| Reviewer | VP Engineering | â€” | 2026-01-17 |
| Approver | CTO | â€” | 2026-01-17 |

---

## Version History

| Version | Date | Author | Changes |
|---------|------|--------|---------|
| v1 | 2026-01-17 | Engineering Operations | Initial release |

---

## Related Documents

| Document | Relationship |
|----------|--------------|
| Platform Reliability & SRE Standard | SRE practices governing this policy |
| Business Continuity Plan | RTO/RPO alignment with durability SLOs |
| Backup & Disaster Recovery Standard | Durability SLO backing infrastructure |
| Observability & Telemetry Standard | Metrics collection |
| Incident Response Standard | Breach handling |
| Change Management Policy | Change-related SLO impact |
| Customer Terms of Service | SLA legal basis |

---

## Appendix A: Glossary

This glossary defines key terms used throughout the CYBERCUBE Service Level
Policy. All definitions are normative unless stated otherwise.

| Term | Definition |
|------|------------|
| **Availability** | Proportion of time a service is operational. Formula: (Total Time - Downtime) / Total Time Ã— 100% |
| **Alert** | Notification triggered when a metric crosses a threshold (warning or critical) |
| **Breach** | Failure to meet an SLA commitment; triggers service credits and remediation |
| **Burn Rate** | Rate at which error budget is consumed. Formula: Error Rate / (1 - SLO) |
| **Compensating Control** | Alternative measure when primary SLO cannot be met |
| **Credit** | Financial compensation for SLA breach, applied to future billing |
| **Degradation** | Partial service impairment not constituting full outage |
| **Downtime** | Period when a service is unavailable or significantly degraded (planned or unplanned) |
| **Error Budget** | Allowable unreliability within an SLO period. Formula: (1 - SLO) Ã— Time Period |
| **Error Budget Policy** | Rules governing actions when error budget is depleted |
| **Exclusion** | Events not counted against SLA calculations (e.g., planned maintenance, force majeure) |
| **Force Majeure** | Unforeseeable circumstances preventing SLA fulfillment |
| **Incident** | Unplanned event causing service degradation or outage (SEV-1 through SEV-4) |
| **Latency** | Time taken to respond to a request, measured at p50, p90, p95, p99 |
| **MTBF** | Mean Time Between Failures. Formula: Total Uptime / Number of Failures |
| **MTTR** | Mean Time to Recovery. Formula: Total Downtime / Number of Incidents |
| **Measurement Window** | Time period over which SLIs are calculated (rolling 30 days or calendar month) |
| **Nines** | Shorthand for availability (e.g., 99.9% = "three nines", 99.99% = "four nines") |
| **Percentile** | Statistical measure for latency distribution (p50 median, p95, p99) |
| **Planned Maintenance** | Scheduled downtime for updates or improvements, per SLA notice terms |
| **RPO** | Recovery Point Objective â€” maximum acceptable data loss measured in time |
| **RTO** | Recovery Time Objective â€” maximum acceptable time to restore service |
| **Service Credit** | Compensation issued for SLA breach, per SLA terms |
| **SLA** | Service Level Agreement â€” contractual commitment to customers (SLOs + consequences) |
| **SLI** | Service Level Indicator â€” quantitative measure of service behavior |
| **SLO** | Service Level Objective â€” internal target for performance (stricter than SLA) |
| **Throughput** | Rate of successful operations, measured in requests per second |
| **Toil** | Repetitive, manual operational work reduced by healthy error budgets |
| **Uptime** | Time when service is available (opposite of downtime) |
