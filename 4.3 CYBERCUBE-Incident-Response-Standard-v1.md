CYBERCUBE Incident Response & Reliability Standard (v1)

Glossary

This glossary defines key terms used throughout the CYBERCUBE Incident Response &
Reliability Standard.

All definitions are normative unless stated otherwise.

A

Action Item

A specific task identified during or after an incident that must be completed.

Properties:
- Owner assigned
- Due date set
- Priority ranked
- Tracked to completion

After Action Review (AAR)

Synonym for Postmortem.

Alert

An automated notification triggered by monitoring systems.

Components:
- Condition (what triggered)
- Severity (urgency)
- Channel (delivery method)
- Runbook (response guide)

Availability

The percentage of time a service is operational.

Formula: `(total_time - downtime) / total_time Ã— 100`

Target: 99.9% (three nines) = ~8.76 hours downtime/year

B

Blameless

A postmortem culture focusing on systemic improvements rather than individual fault.

Principles:
- Assume good intent
- Focus on systems, not people
- Learn from failures
- Prevent recurrence

C

Communication Lead

The incident role responsible for stakeholder updates.

Responsibilities:
- Status page updates
- Customer communications
- Internal notifications
- Executive briefings

Customer Impact

The effect of an incident on customers.

Categories:
- No impact (internal only)
- Degraded experience
- Partial outage
- Complete outage

D

Degradation

Reduced service performance or functionality.

Examples:
- Slow response times
- Partial feature availability
- Increased error rates

Detection Time

Time from incident start to first alert or report.

Also known as: Time to Detect (TTD)

Downtime

Period when a service is completely unavailable.

Measured in minutes or hours.

E

Escalation

Involving additional resources or authority in incident response.

Types:
- Functional (more expertise)
- Hierarchical (management)
- External (vendors, partners)

Escalation Path

A defined sequence of contacts for incident escalation.

Components:
- Primary responder
- Backup responder
- Team lead
- Engineering manager
- VP/CTO

I

Impact

The business or customer effect of an incident.

Factors:
- Number of users affected
- Revenue impact
- Data integrity
- Compliance implications

Incident

An unplanned event disrupting or threatening to disrupt service operation.

Properties:
- Has start/end time
- Has severity level
- Requires response
- Generates postmortem (SEV-1/2)

Incident Commander (IC)

The person coordinating incident response.

Responsibilities:
- Coordinate responders
- Make decisions
- Track timeline
- Ensure communication

Incident ID

Unique identifier for tracking an incident.

Format: `INC-{YYYYMMDD}-{sequence}`

Example: `INC-20260117-001`

Incident Response

The process of detecting, analyzing, and resolving incidents.

Phases:
1. Detection
2. Triage
3. Response
4. Resolution
5. Postmortem

M

Mean Time Between Failures (MTBF)

Average time between incidents.

Higher is better.

Mean Time to Acknowledge (MTTA)

Average time from alert to first human response.

Target: < 5 minutes (SEV-1)

Mean Time to Detect (MTTD)

Average time from incident start to detection.

Target: < 5 minutes

Mean Time to Recovery (MTTR)

Average time from incident detection to resolution.

Targets: See Â§8.1 Key Metrics for canonical MTTR targets by severity.

Mean Time to Resolve

Synonym for MTTR.

Mitigation

Actions taken to reduce incident impact before full resolution.

Examples:
- Rollback deployment
- Enable fallback
- Scale resources
- Block bad actors

N

Near Miss

An event that could have caused an incident but didn't.

Should be reviewed like incidents.

O

On-Call

The practice of having designated responders available 24/7.

Components:
- Primary on-call
- Secondary on-call
- Rotation schedule
- Escalation policy

Outage

Complete loss of service availability.

More severe than degradation.

P

Page

An urgent alert requiring immediate response.

Delivery: PagerDuty, phone call, SMS

Postmortem

A documented review of an incident to prevent recurrence.

Synonym: After Action Review, Retrospective, Incident Review

Components:
- Timeline
- Impact assessment
- Root cause analysis
- Action items

Primary On-Call

The first responder for incidents during their rotation.

Priority

The urgency of addressing an issue.

Related to but distinct from severity.

R

Recovery

Returning service to normal operation.

Types:
- Full recovery (normal state)
- Partial recovery (degraded state)
- Workaround (temporary fix)

Remediation

Permanent fixes implemented after an incident.

Tracked as action items from postmortem.

Resolution

The point when service returns to normal operation.

Marks the end of the incident.

Responder

Anyone actively working on incident resolution.

Types:
- Incident Commander
- Subject Matter Expert
- Communication Lead
- Scribe

Root Cause

The fundamental reason an incident occurred.

Note: Often multiple contributing factors exist.

Runbook

A documented procedure for responding to specific alerts or incidents.

Components:
- Symptoms
- Diagnostic steps
- Resolution steps
- Escalation criteria

S

Scribe

The incident role responsible for documenting the timeline.

Responsibilities:
- Record timeline
- Document decisions
- Capture actions
- Prepare postmortem draft

Severity

The impact level of an incident.

CYBERCUBE levels:
- SEV-1: Critical
- SEV-2: Major
- SEV-3: Minor
- SEV-4: Low

SLA (Service Level Agreement)

A contract defining service expectations.

Components:
- Uptime commitment
- Response times
- Support hours
- Penalties

SLI (Service Level Indicator)

A metric measuring service performance.

Examples:
- Availability percentage
- Latency percentile
- Error rate

SLO (Service Level Objective)

Internal targets for service performance.

More stringent than SLA.

Stakeholder

Anyone with interest in incident status.

Types:
- Customers
- Leadership
- Support team
- Partner teams

Status Page

Public or internal page showing service health.

Updates:
- Investigating
- Identified
- Monitoring
- Resolved

T

Timeline

A chronological record of incident events.

Includes:
- Detection time
- Key actions
- State changes
- Resolution time

Triage

The process of assessing incident severity and assigning resources.

Steps:
1. Assess impact
2. Assign severity
3. Notify responders
4. Begin response

W

War Room

A dedicated communication channel for incident response.

Types:
- Slack channel (#incident-{id})
- Video bridge
- Physical room

---

CYBERCUBE Incident Response & Reliability Standard (v1)

**Standard ID:** STD-OPS-003  
**Status:** Active  
**Effective:** 2026-01-17  
**Classification:** INTERNAL  
**Owner:** SRE / Platform Engineering  
**Approver:** VP Engineering  
**Applies to:** All CYBERCUBE production services and teams  
**Review Cycle:** Annual + after any SEV-1 incident

0. Purpose & Design Principles

This standard defines how CYBERCUBE detects, responds to, and learns from incidents.
It establishes severity classifications, response procedures, and postmortem requirements.

Industry alignment:
- Google SRE Practices
- Atlassian Incident Management Handbook
- PagerDuty Incident Response Guide
- NIST SP 800-61 (Incident Handling)

Design principles:

1. **Customer First** â€” Minimize customer impact above all
2. **Speed** â€” Detect and respond quickly
3. **Clarity** â€” Clear roles, communication, and process
4. **Blamelessness** â€” Focus on systems, not individuals
5. **Learning** â€” Every incident improves the system
6. **Preparation** â€” Practice and prepare before incidents

This document does NOT define:
- Security incident handling â€” see Security Incident Standard
- Alerting configuration â€” see Observability Standard
- On-call compensation â€” see HR policies

1. Severity Classification

Incidents are classified by severity to determine response urgency and resources.

1.1 Severity Levels

| Severity | Name | Impact | Response Time | Example |
|----------|------|--------|---------------|---------|
| SEV-1 | Critical | Complete outage, data loss, security breach | Immediate (< 5 min) | Platform down, data breach |
| SEV-2 | Major | Significant degradation, major feature down | < 15 minutes | Payment processing down |
| SEV-3 | Minor | Limited impact, workaround available | < 1 hour | Single customer affected |
| SEV-4 | Low | Minimal impact, no urgency | Next business day | Cosmetic issue, minor bug |

1.2 Severity Decision Matrix

| Factor | SEV-1 | SEV-2 | SEV-3 | SEV-4 |
|--------|-------|-------|-------|-------|
| Users affected | All / Most | Many | Few | Single |
| Revenue impact | Significant | Moderate | Minor | None |
| Data at risk (see 3.3) | Yes | Possible | No | No |
| Security breach | Yes | Suspected | No | No |
| Workaround | None | Difficult | Available | Easy |
| SLA violation | Imminent | Possible | Unlikely | No |

1.3 Severity Examples

**SEV-1 (Critical):**
- Platform completely unavailable
- Authentication system down
- Data breach confirmed
- Payment system not processing
- Data corruption detected
- All customers affected

**SEV-2 (Major):**
- Major feature unavailable (projects, billing)
- Significant performance degradation (>10x latency)
- >25% of requests failing
- Important integration down
- Subset of customers affected

**SEV-3 (Minor):**
- Single feature degraded
- Few customers affected
- Workaround available
- Non-critical service down
- Intermittent errors

**SEV-4 (Low):**
- Cosmetic/UI issues
- Single customer reporting
- Documentation errors
- Non-urgent improvements
- Internal tooling issues

1.4 Severity Adjustment

Severity can be adjusted during an incident:

**Upgrade triggers:**
- Impact spreading
- Mitigation failing
- Duration extending
- New symptoms discovered

**Downgrade triggers:**
- Mitigation successful
- Impact contained
- Workaround found
- Customer communication clear

Document severity changes in timeline.

2. Incident Lifecycle

Every incident follows a defined lifecycle from detection to closure.

2.1 Lifecycle Phases

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        INCIDENT LIFECYCLE                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ DETECT  â”‚â”€â”€â”€â–¶â”‚ TRIAGE  â”‚â”€â”€â”€â–¶â”‚ RESPOND  â”‚â”€â”€â”€â–¶â”‚   RESOLVE   â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚       â”‚              â”‚               â”‚                â”‚             â”‚
â”‚       â–¼              â–¼               â–¼                â–¼             â”‚
â”‚   Alert fires    Assess        Mitigate &       Service            â”‚
â”‚   or report      severity      diagnose         restored           â”‚
â”‚   received       & notify                                          â”‚
â”‚                                                                     â”‚
â”‚                                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚                                      â”‚  POSTMORTEM â”‚               â”‚
â”‚                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                             â”‚                       â”‚
â”‚                                             â–¼                       â”‚
â”‚                                      Document &                     â”‚
â”‚                                      action items                   â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

2.2 When to Declare an Incident

Not every issue is an incident. Declare an incident when ANY of the following are true:

**Declare immediately:**
- Customer-facing service is degraded or unavailable
- SLO is breached or at imminent risk of breach
- Data loss, corruption, or unauthorized access is suspected
- Multiple customers report the same issue
- Automated monitoring triggers a SEV-1 or SEV-2 alert

**Declare after assessment (within 15 minutes):**
- A single customer reports impact with no obvious workaround
- Internal tooling outage blocks customer-impacting work
- A deployment causes unexpected error rate increase
- Third-party dependency failure affects production

**NOT an incident (use normal ticket workflow):**
- Bug affecting a single user with easy workaround
- Cosmetic/UI issues with no functional impact
- Internal tooling inconvenience with workaround
- Feature request or improvement suggestion

When in doubt, declare an incident. It is always better to declare and downgrade than to miss a real incident.

2.3 Phase 1: Detection

**Goal:** Identify incidents as quickly as possible.

Sources:
- Automated monitoring alerts
- Customer reports
- Internal reports
- Partner notifications
- Third-party monitoring

Detection SLOs:
| Severity | Target MTTD |
|----------|-------------|
| SEV-1 | < 5 minutes |
| SEV-2 | < 10 minutes |
| SEV-3 | < 30 minutes |
| SEV-4 | < 4 hours |

2.4 Phase 2: Triage

**Goal:** Assess severity and mobilize response.

Steps:
1. Acknowledge alert (MTTA < 5 min for SEV-1/2)
2. Assess impact and symptoms
3. Assign initial severity
4. Create incident channel
5. Page appropriate responders
6. Begin timeline

Triage questions:
- What service is affected?
- How many users impacted?
- Is there data loss risk?
- Is there a security concern?
- What is customer-visible impact?
- Is there a workaround?

2.5 Phase 3: Response

**Goal:** Mitigate impact and diagnose root cause.

Parallel tracks:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MITIGATION    â”‚     â”‚   DIAGNOSIS     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Rollback      â”‚     â”‚ â€¢ Review logs   â”‚
â”‚ â€¢ Failover      â”‚     â”‚ â€¢ Check metrics â”‚
â”‚ â€¢ Scale         â”‚     â”‚ â€¢ Trace requestsâ”‚
â”‚ â€¢ Block traffic â”‚     â”‚ â€¢ Review changesâ”‚
â”‚ â€¢ Enable cache  â”‚     â”‚ â€¢ Test theories â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Mitigation first, then diagnose:
- Restore service before root cause
- Don't let perfect be enemy of good
- Workarounds are acceptable

2.6 Phase 4: Resolution

**Goal:** Confirm service is restored and stable.

Resolution criteria:
- Service metrics normal
- No elevated errors
- Customer-confirmed working
- Stable for 15+ minutes
- Monitoring in place

Post-resolution:
1. Update status page (Resolved)
2. Send customer notification
3. Notify stakeholders
4. Schedule postmortem
5. Close incident channel (after postmortem)

2.7 Phase 5: Postmortem

**Goal:** Learn from the incident and prevent recurrence.

Required for: SEV-1, SEV-2
Optional for: SEV-3, SEV-4

Timeline:
- Draft: Within 48 hours
- Review: Within 1 week
- Action items assigned: Within 1 week
- Publish: Within 2 weeks

3. Incident Roles

Clear roles ensure effective response coordination.

3.1 Role Summary

| Role | Responsibility | Required For |
|------|----------------|--------------|
| Incident Commander | Coordinate response | SEV-1, SEV-2 |
| Subject Matter Expert | Technical investigation | All |
| Communication Lead | Stakeholder updates | SEV-1, SEV-2 |
| Scribe | Document timeline | SEV-1, SEV-2 |
| On-Call Responder | Initial response | All |

3.2 Incident Commander (IC)

**Primary responsibility:** Coordinate the response, make decisions, keep things moving.

Tasks:
- Assign roles to responders
- Coordinate parallel workstreams
- Make go/no-go decisions
- Manage escalations
- Call all-hands if needed
- Ensure communication happens
- Track time and progress

Authority:
- Page anyone needed
- Authorize emergency changes
- Override normal processes
- Escalate to leadership

NOT responsible for:
- Hands-on debugging (delegate)
- Writing code (delegate)
- Customer communication (delegate to Comm Lead)

3.3 Subject Matter Expert (SME)

**Primary responsibility:** Technical investigation and remediation.

Tasks:
- Investigate symptoms
- Propose mitigations
- Implement fixes
- Verify resolution
- Document technical details

May include:
- Service owner
- Database expert
- Infrastructure engineer
- Security specialist

3.4 Communication Lead

**Primary responsibility:** Keep stakeholders informed.

Tasks:
- Update status page
- Draft customer notifications
- Brief leadership
- Coordinate support team
- Manage external inquiries

Communication cadence:
| Severity | Update Frequency |
|----------|------------------|
| SEV-1 | Every 15 minutes |
| SEV-2 | Every 30 minutes |
| SEV-3 | Every 2 hours |

3.5 Scribe

**Primary responsibility:** Document the incident timeline.

Tasks:
- Record all significant events
- Note decisions and rationale
- Capture action items
- Prepare postmortem draft
- Track metrics (MTTD, MTTR)

Timeline entries include:
- Timestamp (UTC)
- What happened
- Who took action
- Outcome

3.6 On-Call Responder

**Primary responsibility:** Initial response and triage.

Tasks:
- Acknowledge alerts
- Perform initial triage
- Escalate if needed
- Begin mitigation
- Recruit IC if SEV-1/2

4. On-Call Operations

On-call ensures 24/7 coverage for incident response.

4.1 On-Call Structure

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ON-CALL STRUCTURE              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                             â”‚
â”‚  Primary On-Call (Team A)                   â”‚
â”‚       â”‚                                     â”‚
â”‚       â”‚ No response (15 min)                â”‚
â”‚       â–¼                                     â”‚
â”‚  Secondary On-Call (Team A)                 â”‚
â”‚       â”‚                                     â”‚
â”‚       â”‚ No response (15 min)                â”‚
â”‚       â–¼                                     â”‚
â”‚  Team Lead                                  â”‚
â”‚       â”‚                                     â”‚
â”‚       â”‚ Escalation needed                   â”‚
â”‚       â–¼                                     â”‚
â”‚  Engineering Manager â†’ VP Engineering       â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

4.2 On-Call Rotations

| Team | Rotation Length | Coverage |
|------|-----------------|----------|
| Platform | 1 week | 24/7 |
| API | 1 week | 24/7 |
| Security | 1 week | 24/7 |
| Support | 8 hours | Business hours |

Rotation rules:
- Handoff on Monday 10:00 UTC
- Primary and secondary for each rotation
- No back-to-back weeks without consent
- Swap requests: 48 hours notice minimum
- Holiday coverage: planned 2 weeks ahead

Small team clause:
- Teams with fewer than 4 engineers MAY combine into a shared on-call rotation
- Minimum 3 people per rotation to ensure sustainable coverage
- Combined rotations must still meet response time targets in Â§4.3
- Document shared rotation responsibilities in on-call handoff notes

4.3 On-Call Expectations

**Response times:**
| Alert Severity | Acknowledge | Triage |
|----------------|-------------|--------|
| SEV-1 | 5 minutes | 15 minutes |
| SEV-2 | 10 minutes | 30 minutes |
| SEV-3 | 30 minutes | 2 hours |
| SEV-4 | 4 hours | Next business day |

**Availability requirements:**
- Reachable via phone/PagerDuty
- Internet access for investigation
- Laptop available
- Within 15 minutes of starting work
- Alert on phone at all times

**NOT expected:**
- Work on non-urgent items
- Be at computer constantly
- Skip personal activities
- Work through illness (swap instead)

4.4 On-Call Handoff

Handoff checklist:
- [ ] Review open incidents
- [ ] Review recent changes (deploys, configs)
- [ ] Review any ongoing issues
- [ ] Confirm contact information current
- [ ] Test pager delivery
- [ ] Update on-call schedule

Handoff document:
```markdown
# On-Call Handoff: 2026-01-17

## Outgoing: Alice
## Incoming: Bob

## Open Items
- INC-20260115-003: Monitoring intermittent API latency (SEV-4)
- Deploy of billing-service v2.3 scheduled Tuesday

## Recent Incidents
- INC-20260116-001 (SEV-2): Database failover, resolved

## Known Issues
- Redis cluster showing elevated memory (ticket #1234)

## Notes
- Customer ACME Corp escalated, support tracking
```

4.5 On-Call Health

Track on-call quality:
| Metric | Target |
|--------|--------|
| Pages per week | < 10 |
| Night pages per week | < 2 |
| MTTA | < 5 minutes |
| False positive rate | < 20% |

Improvement actions:
- Tune noisy alerts
- Automate common responses
- Improve runbooks
- Add monitoring gaps

5. Escalation

Escalation brings additional resources or authority to incident response.

5.1 Escalation Types

| Type | When | Who |
|------|------|-----|
| Functional | Need expertise | SME, other team |
| Hierarchical | Need authority | Manager, Director, VP |
| External | Vendor issue | Vendor support, partners |

5.2 Escalation Triggers

**Escalate when:**
- Incident not improving after 30 minutes
- Beyond responder's expertise
- Need authorization for changes
- Customer or business escalation
- Potential PR/legal issue
- Security incident suspected
- Data loss possible

**Don't hesitate to escalate:**
- Better to over-escalate than under
- No penalty for escalating
- IC makes the call

5.3 Escalation Paths

**Technical escalation:**
```
On-Call â†’ Team Lead â†’ Service Owner â†’ Principal Engineer â†’ CTO
```

**Business escalation:**
```
Support â†’ Support Manager â†’ Account Manager â†’ VP Customer Success â†’ CEO
```

**Security escalation:**
```
On-Call â†’ Security Team â†’ Security Lead â†’ CISO â†’ CEO/Legal
```

5.4 Escalation Contacts

| Role | Contact Method | Response Time |
|------|----------------|---------------|
| On-Call | PagerDuty | 5 minutes |
| Team Lead | PagerDuty + Slack | 10 minutes |
| Engineering Manager | Phone | 15 minutes |
| VP Engineering | Phone | 15 minutes |
| CTO | Phone | 15 minutes |
| CEO | Phone | 30 minutes |

Emergency contacts maintained in:
- PagerDuty escalation policies
- Incident runbook (offline copy)
- Physical emergency binder

5.5 Vendor Escalation

For third-party dependencies:

| Vendor | Support Level | Contact |
|--------|---------------|---------|
| AWS | Enterprise Support | TAM + Support case |
| Stripe | Priority Support | Dashboard + phone |
| Datadog | Premium Support | Chat + phone |
| [Others] | Per contract | [Details] |

Vendor escalation checklist:
- [ ] Open support case immediately
- [ ] Request severity upgrade if needed
- [ ] Ask for dedicated engineer
- [ ] Escalate through TAM if available
- [ ] Document case numbers

5.6 BCP/DR Escalation Criteria

An incident triggers Business Continuity (4.1) or Disaster Recovery (4.2) activation when:

| Trigger | Threshold | Activates |
|---------|-----------|-----------|
| Service RTO breach | Incident duration exceeds service RTO | DR failover procedures (4.2) |
| Multi-region failure | Primary AND secondary regions impacted | DR â€” regional failover (4.2) |
| Confirmed data loss | Unrecoverable data corruption or loss | DR â€” backup restore (4.2) |
| Facility unavailable | Office/datacenter inaccessible | BCP â€” alternate site (4.1) |
| Key personnel unavailable | >50% of critical team unreachable | BCP â€” succession plan (4.1) |
| Extended outage | SEV-1 unresolved >4 hours | BCP crisis management team activated (4.1) |

Process:
1. IC identifies BCP/DR trigger condition
2. IC escalates to VP Engineering + BCP Coordinator
3. BCP Coordinator makes activation decision
4. Incident response continues in parallel with BCP/DR procedures
5. Unified communication â€” IC coordinates with BCP Coordinator on stakeholder updates

Cross-references:
- Business Continuity Plan (4.1) â€” activation criteria, crisis team
- Backup & Disaster Recovery Standard (4.2) â€” failover, restore procedures

6. Communication

Clear communication is critical during incidents.

6.1 Regulatory & Privacy Notification

If an incident involves exposure, loss, or unauthorized access to personal data:

1. **Immediately** notify the Privacy/Compliance lead
2. Invoke Privacy Handling Policy (3.2) assessment procedures
3. Assess notification obligations per DPA Standard (3.6)
4. Document data categories affected per Data Classification Standard (3.3)

Regulatory timelines that may apply:
- **GDPR:** 72-hour notification to supervisory authority
- **US State Breach Laws:** Varies (24hâ€“60 days depending on state)
- **Contractual (DPA):** Per customer DPA terms

The Communication Lead coordinates with Legal/Compliance on any external regulatory notification. This runs in parallel with â€” not instead of â€” standard incident response.

6.2 Communication Channels

| Channel | Purpose | Audience |
|---------|---------|----------|
| #incident-{id} | Response coordination | Responders |
| #incidents | Awareness | Engineering |
| Status page | Customer notification | Customers |
| Email | Formal notification | Stakeholders |
| Bridge call | Voice coordination | Responders (SEV-1) |

6.3 Incident Channel Setup

Create immediately on SEV-1/2:

Channel name: `#incident-{date}-{short-description}`

Example: `#incident-20260117-api-outage`

Initial message:
```
ğŸš¨ INCIDENT DECLARED ğŸš¨
Severity: SEV-1
Summary: API returning 503 errors
Impact: All customers affected
IC: @alice
Status: Investigating

Timeline and updates in thread ğŸ‘‡
```

6.4 Status Page Updates

Update flow:
```
1. Investigating (initial)
   "We are investigating reports of..."

2. Identified (cause known)
   "We have identified the cause..."

3. Monitoring (fix deployed)
   "A fix has been deployed. We are monitoring..."

4. Resolved (confirmed stable)
   "This incident has been resolved..."
```

Status page components:
- Overall status (Operational / Degraded / Outage)
- Component status (API, Dashboard, Billing, etc.)
- Incident updates (timestamped)

6.5 Customer Communication Templates

**Initial notification (SEV-1/2):**
```
Subject: [CYBERCUBE] Service Disruption - {Component}

We are currently experiencing issues with {component/service}.

Impact: {What customers are experiencing}

Our team is actively investigating and working to resolve this issue.
We will provide updates every {15/30} minutes.

For questions, contact support@cybercube.software.
```

**Update notification:**
```
Subject: [UPDATE] {Component} Service Disruption

Update: {What we learned or did}

Current Status: {Investigating/Identified/Monitoring}

Next Update: {Time}
```

**Resolution notification:**
```
Subject: [RESOLVED] {Component} Service Disruption

The service disruption affecting {component} has been resolved.

Duration: {Start time} to {End time} ({duration})
Impact: {Summary of impact}

Root Cause: {Brief explanation}

We apologize for any inconvenience. A detailed postmortem 
will be published within {timeframe}.
```

6.6 Internal Communication

**Stakeholder updates (SEV-1/2):**

Recipients: Leadership, Support, Customer Success

Frequency:
- SEV-1: Every 15 minutes
- SEV-2: Every 30 minutes

Format:
```
INCIDENT UPDATE - INC-20260117-001 - SEV-1 - API Outage

Time: 14:30 UTC
Duration: 45 minutes
Status: Mitigating

Current Impact:
- API returning 503 errors
- Dashboard inaccessible
- ~500 customers affected

Actions Taken:
- Identified: Database connection exhaustion
- Mitigation: Scaled connection pool
- Status: Monitoring recovery

Next Update: 14:45 UTC
IC: Alice (alice@cybercube.software)
```

7. Postmortem Process

Postmortems document incidents and drive improvement.

7.1 Postmortem Requirements

| Severity | Postmortem Required | Timeline |
|----------|---------------------|----------|
| SEV-1 | Yes | Draft within 48 hours |
| SEV-2 | Yes | Draft within 1 week |
| SEV-3 | Optional | If significant learning |
| SEV-4 | No | Unless requested |

7.2 Postmortem Template

```markdown
# Postmortem: {Incident Title}

**Incident ID:** INC-20260117-001
**Date:** 2026-01-17
**Duration:** 2 hours 15 minutes
**Severity:** SEV-1
**Authors:** Alice, Bob
**Status:** Final

## Executive Summary
{2-3 sentence summary of what happened and impact}

## Impact
- **Duration:** 13:15 UTC - 15:30 UTC (2h 15m)
- **Users Affected:** ~2,500 (all production users)
- **Revenue Impact:** ~$X in processing delayed
- **SLA Impact:** Below 99.9% target for January
- **Customer Tickets:** 47 tickets opened

## Timeline (UTC)
| Time | Event |
|------|-------|
| 13:15 | Database primary begins showing connection errors |
| 13:18 | Automated alert fires: db-connection-exhaustion |
| 13:20 | On-call acknowledges, begins investigation |
| 13:25 | SEV-1 declared, incident channel created |
| 13:30 | IC assigned (Alice), team mobilized |
| 13:45 | Root cause identified: connection leak in billing-service v2.3 |
| 14:00 | Mitigation: connection pool size increased |
| 14:15 | Partial recovery observed |
| 14:30 | Decision to rollback billing-service to v2.2 |
| 14:45 | Rollback complete |
| 15:00 | Full service recovery confirmed |
| 15:30 | Incident resolved, monitoring continues |

## Root Cause
The billing-service v2.3 release (deployed 12:00 UTC) introduced a 
connection leak in the payment processing flow. Under load, connections 
were not properly returned to the pool, exhausting the database 
connection limit (500 connections) within 75 minutes.

## Contributing Factors
1. Connection leak not caught in code review
2. Load testing did not simulate production transaction volume
3. Connection pool monitoring alert threshold too high
4. No circuit breaker on database connections

## Resolution
- Immediate: Rolled back billing-service to v2.2
- Connection pool manually reset

## What Went Well
- Alert fired within 3 minutes of impact
- Team mobilized quickly
- Clear communication to customers
- Rollback process worked smoothly

## What Went Wrong
- Connection leak not caught before production
- Initial mitigation (pool size increase) insufficient
- Took 30 minutes to decide on rollback
- Load testing inadequate

## Lessons Learned
1. Need connection lifecycle testing in CI/CD
2. Need lower threshold on connection alerts
3. Need automated rollback capability
4. Need circuit breaker pattern for DB connections

## Action Items
| ID | Action | Owner | Due Date | Priority |
|----|--------|-------|----------|----------|
| 1 | Add connection leak test to CI | Bob | 2026-01-24 | P1 |
| 2 | Lower connection alert threshold to 80% | Carol | 2026-01-20 | P1 |
| 3 | Implement circuit breaker for DB | Bob | 2026-02-01 | P2 |
| 4 | Review load test scenarios | Alice | 2026-01-31 | P2 |
| 5 | Add automated rollback for critical services | DevOps | 2026-02-15 | P2 |

## Appendix
- [Incident Slack channel archive](#)
- [Monitoring dashboard during incident](#)
- [Related code change](#)
```

7.3 Postmortem Process

```
Incident Resolved
      â†“
Schedule postmortem meeting (within 48h for SEV-1)
      â†“
Scribe drafts postmortem (using template)
      â†“
Gather data: logs, metrics, timeline
      â†“
Postmortem meeting:
  - Review timeline
  - Identify root cause
  - Discuss contributing factors
  - Generate action items
      â†“
Finalize document
      â†“
Review with stakeholders
      â†“
Publish to postmortem repository
      â†“
Track action items to completion
      â†“
Close incident
```

7.4 Blameless Postmortem Culture

**DO:**
- Focus on systems and processes
- Ask "what" not "who"
- Assume everyone acted with good intent
- Look for systemic improvements
- Celebrate learning
- Share widely

**DON'T:**
- Blame individuals
- Punish for mistakes
- Hide incidents
- Skip postmortems
- Ignore action items
- Repeat the same failures

Questions that help:
- "What information was missing?"
- "What could we automate?"
- "What monitoring would help?"
- "What would have prevented this?"

7.5 Action Item Tracking

Action item requirements:
- Clear owner
- Specific due date
- Priority (P1-P4)
- Link to incident
- Definition of done

Priority guidelines:
| Priority | Timeline | Description |
|----------|----------|-------------|
| P1 | 1 week | Critical prevention |
| P2 | 1 month | Important improvement |
| P3 | 1 quarter | Nice to have |
| P4 | Backlog | Future consideration |

Tracking:
- Track in incident management system
- Review weekly in team meeting
- Monthly incident review includes action item status
- Close only when verified complete

Accountability:
- **DRI (Directly Responsible Individual):** Engineering Manager or SRE Lead owns overall action-item tracking
- Items overdue by >1 week: DRI escalates to item owner's manager
- Items overdue by >2 weeks: flagged in monthly reliability review (Â§8.3)
- Completion rate tracked as a team metric (target: >90% on-time for P1/P2)

8. Metrics & Reporting

Track incident metrics to measure and improve reliability.

8.1 Key Metrics

| Metric | Definition | SEV-1 | SEV-2 | SEV-3 | SEV-4 |
|--------|------------|-------|-------|-------|-------|
| MTTD | Time from start to detection | < 5 min | < 10 min | < 30 min | < 4 hours |
| MTTA | Time from alert to acknowledge | < 5 min | < 10 min | < 30 min | < 4 hours |
| MTTR | Time from detection to resolution | < 1 hour | < 4 hours | < 24 hours | Next business day |

| Metric | Definition | Target |
|--------|------------|--------|
| MTBF | Time between incidents | Increasing |
| Incident count | Number of incidents per period | Decreasing |
| Customer impact hours | Users Ã— hours affected | Minimizing |

Service-level SLOs (availability targets per service) are defined in the Observability & Telemetry Standard (4.5) and Platform Reliability Standard (4.4).

8.2 Incident Reports

**Weekly report:**
- Incidents this week (count by severity)
- MTTR by severity
- Action items status
- Notable incidents summary

**Monthly report:**
- Incident trends
- SLA/SLO status
- Top incident categories
- Action item completion rate
- Improvements made

**Quarterly report:**
- Incident trends over time
- Reliability improvements
- Major postmortem learnings
- Investment recommendations

8.3 Reliability Review

Monthly reliability review meeting:

Attendees: Engineering leads, SRE, Product

Agenda:
1. Review incident metrics
2. Review open action items
3. Discuss recurring issues
4. Plan improvements
5. Celebrate wins

9. Tools & Infrastructure

Standard tools for incident management.

9.1 Tool Stack

| Function | Tool | Purpose |
|----------|------|---------|
| Alerting | PagerDuty | On-call paging |
| Monitoring | Datadog | Metrics, logs, traces |
| Status Page | Statuspage.io | Customer communication |
| Communication | Slack | Incident channels |
| Video | Zoom | Bridge calls |
| Documentation | Notion | Postmortems, runbooks |
| Ticketing | Jira | Action item tracking |

9.2 PagerDuty Configuration

Services:
- cybercube-platform (SEV-1/2 routing)
- cybercube-api (API alerts)
- cybercube-database (DB alerts)
- cybercube-security (Security alerts)

Escalation policies:
```
Level 1 (0 min): Primary on-call
Level 2 (15 min): Secondary on-call
Level 3 (30 min): Team lead
Level 4 (45 min): Engineering manager
Level 5 (60 min): VP Engineering
```

9.3 Runbook Repository

Structure:
```
/runbooks
  /alerts
    /high-error-rate.md
    /high-latency.md
    /database-connection-exhausted.md
  /services
    /api-gateway.md
    /billing-service.md
    /database.md
  /procedures
    /rollback.md
    /failover.md
    /scale-up.md
```

Runbook template:
```markdown
# Runbook: {Alert/Service Name}

## Overview
{Brief description}

## Symptoms
- {What you'll see}

## Severity Assessment
{How to determine severity}

## Diagnostic Steps
1. {Step}
2. {Step}

## Resolution Steps
1. {Step}
2. {Step}

## Escalation
{When and to whom}

## Related
- {Links to dashboards}
- {Links to documentation}
```

10. Training & Exercises

Preparation ensures effective incident response when real incidents occur.

10.1 Training Requirements

| Audience | Training | Frequency |
|----------|----------|-----------|
| All engineers | IR process overview | Onboarding + annual |
| On-call engineers | On-call procedures, tooling, runbooks | Before first rotation |
| Incident Commanders | IC role, decision-making, communication | Before serving as IC |
| Communication Leads | Customer comms, status page, stakeholder updates | Before serving as Comm Lead |
| New hires (engineering) | Shadow on-call rotation (1 week) | First month |

10.2 Exercises

| Exercise Type | Scope | Frequency | Participants |
|---------------|-------|-----------|--------------|
| Tabletop exercise | Walk through a simulated incident scenario | Quarterly | On-call engineers, ICs |
| Communication drill | Practice status page + customer notification flow | Semi-annually | Comm Leads, Support |
| On-call handoff review | Verify handoff process and contacts | Each rotation |  Outgoing + incoming on-call |
| Failover test | Validate DR failover per BDR Standard (4.2) | Annually | SRE + Platform team |

Tabletop exercise format:
1. Facilitator presents a scenario (30 min)
2. Team walks through detection, triage, response, communication (45 min)
3. Debrief: what went well, what to improve (15 min)
4. Document findings and update runbooks as needed

10.3 Exercise Tracking

Track in incident management system:
- Exercise date and type
- Participants
- Scenario description
- Findings and gaps identified
- Action items generated

Review exercise results in monthly reliability review (Â§8.3).

---

CYBERCUBE Incident Response â€” Quick Reference Card

Version: v1 | 2026-01-17  
Print it. Keep it handy.

ğŸ”¹ Severity Levels

| SEV | Impact | Response | Example |
|-----|--------|----------|---------|
| 1 | Critical | Immediate | Platform down |
| 2 | Major | 15 min | Feature down |
| 3 | Minor | 1 hour | Few users |
| 4 | Low | Next day | Cosmetic |

ğŸ”¹ Response Times

| Metric | SEV-1 | SEV-2 | SEV-3 |
|--------|-------|-------|-------|
| MTTD | 5 min | 10 min | 30 min |
| MTTA | 5 min | 10 min | 30 min |
| MTTR | 1 hour | 4 hours | 24 hours |

ğŸ”¹ Incident Lifecycle

```
DETECT â†’ TRIAGE â†’ RESPOND â†’ RESOLVE â†’ POSTMORTEM
```

ğŸ”¹ Roles (SEV-1/2)

| Role | Responsibility |
|------|----------------|
| IC | Coordinate response |
| SME | Technical investigation |
| Comm Lead | Stakeholder updates |
| Scribe | Document timeline |

ğŸ”¹ First Response Checklist

```
â–¡ Acknowledge alert
â–¡ Assess severity
â–¡ Create #incident-{date}-{desc}
â–¡ Post initial summary
â–¡ Assign IC (if SEV-1/2)
â–¡ Update status page
â–¡ Begin investigation
```

ğŸ”¹ Status Page States

```
ğŸ” Investigating â†’ ğŸ” Identified â†’ ğŸ‘€ Monitoring â†’ âœ… Resolved
```

ğŸ”¹ Escalation Triggers

- Not improving after 30 min
- Beyond expertise
- Need authorization
- Customer escalation
- Security concern
- Data loss risk

ğŸ”¹ Communication Cadence

| SEV | Update Frequency |
|-----|------------------|
| 1 | Every 15 min |
| 2 | Every 30 min |
| 3 | Every 2 hours |

ğŸ”¹ Postmortem Requirements

| SEV | Required | Timeline |
|-----|----------|----------|
| 1 | Yes | 48 hours |
| 2 | Yes | 1 week |
| 3 | Optional | If learning |
| 4 | No | â€” |

ğŸ”¹ Escalation Path

```
On-Call â†’ Team Lead â†’ Manager â†’ VP â†’ CTO
```

ğŸ”¹ Key Contacts

```
PagerDuty: [policy]
Slack: #incidents
Status: status.cybercube.software
Bridge: [zoom link]
```

ğŸ”¹ Don't Forget

âœ… Mitigate first, diagnose second
âœ… Over-communicate, not under
âœ… Escalate early, not late
âœ… Document everything
âœ… Update status page
âœ… Blameless always

---

## Implementation Status

**Last Updated:** 2026-01-17
**Standard Version:** v1

### Core Implementation

| Component | Status | Notes |
|-----------|--------|-------|
| Severity Classification | PARTIAL | Informal use |
| On-Call Rotation | PARTIAL | Needs formalization |
| PagerDuty Setup | PENDING | Not configured |
| Escalation Policies | PENDING | Document contacts |
| Status Page | PENDING | Select provider |
| Incident Channels | PARTIAL | Manual creation |
| Postmortem Process | PENDING | No template in use |
| Runbook Repository | PENDING | Create structure |
| Metrics Tracking | PENDING | Define dashboards |
| Monthly Reviews | PENDING | Schedule meetings |
| Training & Exercises | PENDING | Schedule quarterly tabletops |
| BCP/DR Integration | PENDING | Align with 4.1/4.2 activation criteria |

### Migration Path

1. **Phase 1**: Severity definitions + on-call formalization
2. **Phase 2**: PagerDuty + escalation setup
3. **Phase 3**: Status page implementation
4. **Phase 4**: Postmortem process + templates
5. **Phase 5**: Runbook repository
6. **Phase 6**: Metrics + reporting
7. **Phase 7**: Training program + quarterly tabletop exercises
8. **Phase 8**: BCP/DR integration alignment with 4.1/4.2

---

Version History

| Version | Date | Changes |
|---------|------|---------|
| v1 | 2026-01-17 | Initial release |
